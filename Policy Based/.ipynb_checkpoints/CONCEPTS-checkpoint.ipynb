{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTHfsOnI2v0U"
   },
   "source": [
    "## Introduction to Poilcy Based methods : Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2szguC-2v0U"
   },
   "source": [
    "Reinforcement learning is utlimately about learning the optimal policy from interaction with the enviroment.\n",
    "\n",
    "In value based methods we first try to find an estimate of optimal value function. For small states spaces this function is represented by a table where each row corrsponds to a state space and column corresponds to each action. To find the optimal policy we find the state action pair with highest value estimate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79D8ra962v0U"
   },
   "source": [
    "If the number of state space is very large like in case of cart pole , we use neural network to get most optimal action based on output of the neural network. But even here we find optimal action after finding optimal value function. But,what if we completely want to skip the step of finding value function.Yes, we can do it and this comes under policy based methods.\n",
    "\n",
    "<img src=\"bad_value_state_example.PNG\" alt=\"J\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-mJY2YJ2v0U"
   },
   "source": [
    "Now consider the cart pole problem for hill climbing. The agent has two possible actions, it mght go left or right based on the position of the pole. We can construct a neural network that can take the state as input and the output of the network can decide the probabilities of taking an action. Our objective here is to then determine the weights of the network for finding the most optimal probabiities of the actions. It is an iterative process, where the weights are amended in each iterations to optimize the weights for finding the most optimal policy.\n",
    "\n",
    "<img src=\"cartpole policy neural.PNG\" alt=\"J\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KedKYpkQ2v0V"
   },
   "source": [
    "### Policy-Based Methods\n",
    "* With value-based methods, the agent uses its experience with the environment to maintain an estimate of the optimal action-value function. The optimal policy is then obtained from the optimal action-value function estimate.\n",
    "* Policy-based methods directly learn the optimal policy, without having to maintain a separate value function estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWXIBWSj2v0V"
   },
   "source": [
    "### Policy Function Approximation\n",
    "* In deep reinforcement learning, it is common to represent the policy with a neural network. \n",
    "** This network takes the environment state as input.\n",
    "** If the environment has discrete actions, the output layer has a node for each possible action and contains the probability that the agent should select each possible action.\n",
    "* The weights in this neural network are initially set to random values. Then, the agent updates the weights as it interacts with (and learns more about) the environment.\n",
    "* Policy-based methods can learn either stochastic or deterministic policies, and they can be used to solve environments with either finite or continuous action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rioNIJs_BeHt"
   },
   "source": [
    "### Hill Climbing\n",
    "* Hill climbing is an iterative algorithm that can be used to find the weights $\\theta$ for an optimal policy.\n",
    "* At each iteration,\n",
    "** We slightly perturb the values of the current best estimate for the weights $\\theta_{best}$, to yield a new set of weights.\n",
    "** These new weights are then used to collect an episode. If the new weights $\\theta_{new}$ resulted in higher return than the old weights, then we set $\\theta_{best} \\leftarrow \\theta_{new}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent's goal is to always maximize the expected return (J). Lets refer to the network weights as $\\theta$. $\\theta$ encodes the policy that makes some actions more likely than others, and those actions influence the reward which affect the expected return value $J$.\n",
    "\n",
    "<img src =\"jtheta_obj_fn.PNG\" alt=\"J and Theta\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We refer to the general class of approaches that find $\\arg\\max_{\\theta}J(\\theta)$ through randomly perturbing the most recent best estimate as stochastic policy search. Likewise, we can refer to J as an objective function, which just refers to the fact that we'd like to maximize it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Consider a case where a neural network has only 2 weights: $\\theta_0$ and $\\theta_1$. Then we can plot the exepected return J as a function of value of both of the weights.\n",
    " Remember here that the agent's goal is to maximize the expected return J. To do so, we first initialize the weights randomly. We collect a single episode with the policy that corresponds to those weights and then record the return. This return is going to be an estimate of what the surface looks like at that value of $\\theta$\n",
    "\n",
    "<img src =\"hill_climbing_first.PNG\" alt=\"J and Theta\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now that we have found the return at this point, we add some noise (say Guassian) to our candidate weights for $\\theta$. To see how good those new weights are, we'll use the policy that they give us to again interact with the environment for an episode and add up the return. If the new weights give us more return than our current best estimate, we focus our attention on our new value.\n",
    "\n",
    "<img src =\"hill_climbing_second.PNG\" alt=\"J and Theta\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then we just repeat by iteratively proposing new policies in the hope that they outperform the existing policy.\n",
    "\n",
    "<img src =\"hill_climbing_third.PNG\" alt=\"J and Theta\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the event that they don't\n",
    "\n",
    "<img src =\"hill_climbing_fourth.PNG\" alt=\"J and Theta\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then we just go back to our last best guess for the optimal policy \n",
    "\n",
    "<img src =\"hill_climbing_fifth.PNG\" alt=\"J and Theta\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* and iterate until we end up with the optimal policy\n",
    "\n",
    "<img src = \"hill_climbing_sixth.PNG\" alt=\"J\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrUP1cfPCiO1"
   },
   "source": [
    "### Beyond Hill Climbing\n",
    "* Steepest ascent hill climbing is a variation of hill climbing that chooses a small number of neighboring policies at each iteration and chooses the best among them.\n",
    "* Simulated annealing uses a pre-defined schedule to control how the policy space is explored, and gradually reduces the search radius as we get closer to the optimal solution.\n",
    "* Adaptive noise scaling decreases the search radius with each iteration when a new best policy is found, and otherwise increases the search radius.\n",
    "* The cross-entropy method iteratively suggests a small number of neighboring policies, and uses a small percentage of the best performing policies to calculate a new estimate.\n",
    "* The evolution strategies technique considers the return corresponding to each candidate policy. The policy estimate at the next iteration is a weighted sum of all of the candidate policies, where policies that got higher return are given higher weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3q3tTUNDDqR"
   },
   "source": [
    "### Why Policy-Based Methods?\n",
    "* There are three reasons why we consider policy-based methods:\n",
    " 1. Simplicity: Policy-based methods directly get to the problem at hand (estimating the optimal policy), without having to store a bunch of additional data (i.e., the action values) that may not be useful.\n",
    " 2. Stochastic policies: Unlike value-based methods, policy-based methods can learn true stochastic policies.\n",
    " 3. Continuous action spaces: Policy-based methods are well-suited for continuous action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdQ9UxiZFCM6"
   },
   "source": [
    "### What are Policy Gradient Methods?\n",
    "* Policy-based methods are a class of algorithms that search directly for the optimal policy, without simultaneously maintaining value function estimates.\n",
    "* Policy gradient methods are a subclass of policy-based methods that estimate the weights of an optimal policy through gradient ascent.\n",
    "In this lesson, we represent the policy with a neural network, where our goal is to find the weights \\thetaÎ¸ of the network that maximize expected return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9Bh0XEMFEzl"
   },
   "source": [
    "### The Big Picture\n",
    "* The policy gradient method will iteratively amend the policy network weights to:\n",
    "make (state, action) pairs that resulted in positive return more likely, and\n",
    "make (state, action) pairs that resulted in negative return less likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f2joI4bFKGN"
   },
   "source": [
    "### Problem Setup\n",
    "* A trajectory $\\tau$ is a state-action sequence $s_0, a_0, \\ldots, s_H, a_H, s_{H+1}$.\n",
    "* In this lesson, we will use the notation $R(\\tau)$ to refer to the return corresponding to trajectory $\\tau$.\n",
    "* Our goal is to find the weights $\\theta$ of the policy network to maximize the expected return $U(\\theta) := \\sum_\\tau \\mathbb{P}(\\tau;\\theta)R(\\tau)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PkJqiBwFYjR"
   },
   "source": [
    "### REINFORCE\n",
    "The pseudocode for REINFORCE is as follows:\n",
    "* Use the policy $\\pi_\\theta$ to collect mm trajectories ${ \\tau^{(1)}, \\tau^{(2)}, \\ldots, \\tau^{(m)}}$ with horizon $H$. We refer to the $i$-th trajectory as\n",
    "$\\tau^{(i)} = (s_0^{(i)}, a_0^{(i)}, \\ldots, s_H^{(i)}, a_H^{(i)}, s_{H+1}^{(i)})$.\n",
    "* Use the trajectories to estimate the gradient $\\nabla_\\theta U(\\theta) :\n",
    "\\nabla_\\theta U(\\theta) \\approx \\hat{g} := \\frac{1}{m}\\sum_{i=1}^m \\sum_{t=0}^{H} \\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)}|s_t^{(i)}) R(\\tau^{(i)})$\n",
    "* Update the weights of the policy:\n",
    "$\\theta \\leftarrow \\theta + \\alpha \\hat{g}$\n",
    " \n",
    "Loop over steps 1-3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ultimately we use Gradient Ascent over a set of state, action and rewards called a Trajectory and find the maximum of the Objective function that help us get our optimal weights for $\\theta$ and $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrwKKpgmFdDX"
   },
   "source": [
    "* REINFORCE can solve Markov Decision Processes (MDPs) with either discrete or continuous action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6l2KNd9FSLK"
   },
   "source": [
    "# References\n",
    " *  The canonical reference for reinforcement learning is the [Reinforcement Learning book by Sutton and Barto](http://www.incompleteideas.net/book/the-book-2nd.html).\n",
    " * Referenced this [paper](https://people.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf) for understanding of REINFORCE \n",
    " * Reference this [blog](https://openai.com/blog/evolution-strategies/) for policy based methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2020 Sonali Vinodkumar Singh\n",
    "\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MINIPROJECT 3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
