{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration for Gird World Game\n",
    "\n",
    "### Abstract: \n",
    "This notebook contains implementation of policy iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "---------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n"
     ]
    }
   ],
   "source": [
    "# Adapted from: https://github.com/lazyprogrammer/machine_learning_examples/tree/master/rl\n",
    "import numpy as np\n",
    "from gridWorldGame import standard_grid, negative_grid,print_values, print_policy\n",
    "\n",
    "SMALL_ENOUGH = 1e-3\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "# this grid gives you a reward of -0.1\n",
    "# to find a shorter path to the goal, use negative grid\n",
    "grid = negative_grid()\n",
    "print(\"rewards:\")\n",
    "print_values(grid.rewards, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state -> action\n",
    "# choose an action and update randomly \n",
    "policy = {}\n",
    "for s in grid.actions.keys():\n",
    "  policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial policy:\n",
      "---------------------------\n",
      "  U  |  D  |  R  |     |\n",
      "---------------------------\n",
      "  D  |     |  U  |     |\n",
      "---------------------------\n",
      "  R  |  U  |  L  |  U  |\n"
     ]
    }
   ],
   "source": [
    "# initial policy\n",
    "print(\"initial policy:\")\n",
    "print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): 0.6630342501731582, (1, 3): 0, (2, 1): 0.5314841752195402, (2, 3): 0.5927590994242267, (1, 0): 0.32297214161807997, (0, 3): 0, (0, 1): 0.5203456179341621, (1, 2): 0.655993276086629, (2, 0): 0.8072789872785315, (2, 2): 0.5584638791364958, (0, 2): 0.18448018291646706}\n",
      "---------------------------\n",
      " 0.66| 0.52| 0.18| 0.00|\n",
      "---------------------------\n",
      " 0.32| 0.00| 0.66| 0.00|\n",
      "---------------------------\n",
      " 0.81| 0.53| 0.56| 0.59|\n"
     ]
    }
   ],
   "source": [
    "# initialize V(s) - value function\n",
    "V = {}\n",
    "states = grid.all_states()\n",
    "for s in states:\n",
    "  # V[s] = 0\n",
    "  if s in grid.actions:\n",
    "    V[s] = np.random.random()\n",
    "  else:\n",
    "    # terminal state\n",
    "    V[s] = 0\n",
    "\n",
    "# initial value for all states in grid\n",
    "print(V)\n",
    "print_values(V, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values 1: \n",
      "---------------------------\n",
      " 0.66| 0.52| 0.18| 0.00|\n",
      "---------------------------\n",
      " 0.32| 0.00| 0.66| 0.00|\n",
      "---------------------------\n",
      " 0.81| 0.53| 0.56| 0.59|\n",
      "policy 1: \n",
      "---------------------------\n",
      "  U  |  D  |  R  |     |\n",
      "---------------------------\n",
      "  D  |     |  U  |     |\n",
      "---------------------------\n",
      "  R  |  U  |  L  |  U  |\n",
      "values 2: \n",
      "---------------------------\n",
      "-0.99|-0.99| 1.00| 0.00|\n",
      "---------------------------\n",
      "-0.99| 0.00| 0.80| 0.00|\n",
      "---------------------------\n",
      "-0.99|-0.99|-0.99|-1.00|\n",
      "policy 2: \n",
      "---------------------------\n",
      "  U  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  R  |  U  |  U  |  L  |\n",
      "values 3: \n",
      "---------------------------\n",
      "-0.99| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      "-0.99| 0.00| 0.80| 0.00|\n",
      "---------------------------\n",
      "-0.99|-0.99| 0.62| 0.46|\n",
      "policy 3: \n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  R  |  R  |  U  |  L  |\n",
      "values 4: \n",
      "---------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.46| 0.00| 0.80| 0.00|\n",
      "---------------------------\n",
      " 0.31| 0.46| 0.62| 0.46|\n",
      "policy 4: \n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "iteration=0\n",
    "# repeat until convergence\n",
    "# when policy does not change, it will finish\n",
    "while True:\n",
    "  iteration+=1\n",
    "  print(\"values %d: \" % iteration)\n",
    "  print_values(V, grid)\n",
    "  print(\"policy %d: \" % iteration)\n",
    "  print_policy(policy, grid)\n",
    "\n",
    "  # policy evaluation step\n",
    "  while True:\n",
    "    biggest_change = 0\n",
    "    for s in states:\n",
    "      old_v = V[s]\n",
    "\n",
    "      # V(s) only has value if it's not a terminal state\n",
    "      if s in policy:\n",
    "        a = policy[s]\n",
    "        grid.set_state(s)\n",
    "        r = grid.move(a) #reward\n",
    "        V[s] = r + GAMMA * V[grid.current_state()]\n",
    "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "    if biggest_change < SMALL_ENOUGH:\n",
    "      break\n",
    "\n",
    "  # policy improvement step\n",
    "  is_policy_converged = True\n",
    "  for s in states:\n",
    "    if s in policy:\n",
    "      old_a = policy[s]\n",
    "      new_a = None\n",
    "      best_value = float('-inf')\n",
    "      # loop through all possible actions to find the best current action\n",
    "      for a in ALL_POSSIBLE_ACTIONS:\n",
    "        grid.set_state(s)\n",
    "        r = grid.move(a)\n",
    "        v = r + GAMMA * V[grid.current_state()]\n",
    "        if v > best_value:\n",
    "          best_value = v\n",
    "          new_a = a\n",
    "      policy[s] = new_a\n",
    "      if new_a != old_a:\n",
    "        is_policy_converged = False\n",
    "\n",
    "  if is_policy_converged:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final values:\n",
      "---------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.46| 0.00| 0.80| 0.00|\n",
      "---------------------------\n",
      " 0.31| 0.46| 0.62| 0.46|\n",
      "final policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "print(\"final values:\")\n",
    "print_values(V, grid)\n",
    "print(\"final policy:\")\n",
    "print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "This is the way of finding an optimal policy, and its called policy iteration. The notebook has poilcy iteration with final values and policy for the gird in the above cell for Grid world game."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
