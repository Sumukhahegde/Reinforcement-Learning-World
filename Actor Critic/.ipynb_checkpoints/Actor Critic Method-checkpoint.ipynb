{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Abhishek Dabas\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract:\n",
    "There are 2 different types of reinforcement Learning methods, \n",
    "- Value based\n",
    "- Policy based\n",
    "\n",
    "- Both of these have some drawbacks, where comes a new method, which is acctually a hybrid method called \"Actor Critic Method\". In this notebook we will try to go through this new Method and try to implement it in one of the Gym Environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Based Methods:\n",
    "**Policy:** A policy is defined as the probability distribution of actions given a state\n",
    "$$P(A|S)$$\n",
    "In Policy-based, there is not need to learn a value function. It select an action without using a value fuction. In this method we directly try to optimze the value function $\\pi$ .\n",
    "- $\\pi$ is the probability distribution of the actions\n",
    "$$\\pi_\\theta(a|s) = P(a|s)$$\n",
    "## There are 2 types of Policy:\n",
    "1. **Deterministic:**\n",
    "- It maps a state to an action. A single action is returned by the policy to be taken. \n",
    "- They are used in deterministic environment. ex chess.\n",
    "1. **Stochastic:**\n",
    "- In stochastic environment we have a probability distribution of the actions. There is a probability we will take a different action. \n",
    "- It is used when an environemtn is uncertain\n",
    "## Advantages:\n",
    "- They have ``better convergence properties.`` Value based methods oscilate alot. In policy based methods we follow a bepolicy gradient, to find the best parameters. Because we follow the gradient here, we are guaranteed to converge with the local maximum or global maximum. \n",
    "- Policy based methods are ``better in high dimensional action spaces.`` When there is continuous action spaces, they work better. In DQN we try to assign a score to the definte action, at each time step, but when the action space is continuous, this becomes very complicated, ex driving a car, where the angle of the wheel 15,15.1, 15.2 etc are possibilities. Policy methods adjust the parameters directly. \n",
    "- Policy based methods can ``learn stochastic policy``. We dont need to implement the exploration/exploitation tradeoff, in this. In stochastic policy the agent explored the state space without always taking the same action. The output space here is a probability distribution over actions. \n",
    "## Disadvantages:\n",
    "- They take ``alot of time to converge, often getting stuck on the local maximum rather than global optimum. `` They take slow step by step\n",
    "- ``Evaluating a policy is inefficient and has high variance``\n",
    "## Check if the policy is Good or Not\n",
    "TTo measure how good a policy is we use a function called,`` Objective function`` that calculates the expected reward tof the policy. In Policy based methods we are trying to optimize the best parameters($\\theta$). $J\\theta$ will tell us how good the policy is and the policy ascent will help us find the best policy parameters to maximize the good actions\n",
    "$$J(\\theta) = E_(\\pi\\theta) [\\sum \\gamma r]$$\n",
    "- we want to check the quality of the policy $\\pi$ with a score function $J(\\theta)$\n",
    "- Use policy gradient ascent to find the best parameters $\\theta$ that improves $\\pi$\n",
    "### Policy gradient Ascent \n",
    "Once we know how good our policy is, we want to maximize the parameters $\\theta$ that maximizes the score function. Maximizing this score function means finding the optimal policy.  Now, for maximizing this score function $J(\\theta)$, we do gradient ascent on policy parameters. Gradient ascent is just the inverse of gradient descent. In gradient ascent we take the direction of the steepest ascent. We want to find the gradient to the current policy $\\pi$ that updates the parameters in the direction of greatest increase, and then iterate. \n",
    "$$Policy : \\pi_\\theta$$\n",
    "$$Pbjective function : J(\\theta)$$\n",
    "$$Gradient : \\triangledown_\\theta J(\\theta)$$\n",
    "$$Update : \\theta \\leftarrow \\theta + \\alpha \\triangledown_\\theta J(\\theta)$$\n",
    "we want to find the policy that maximizes the score:\n",
    "$$\\theta^* = argmax J(\\theta) =  argmax E_(\\pi \\theta) [\\sum R(s_t,a_t]$$\n",
    "which is the total summation of expected rewar given policy. So we want to differenciate the sore function $J(\\theta)$\n",
    "- Example: [Cartpole](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Policy%20Gradients/Cartpole/Cartpole%20REINFORCE%20Monte%20Carlo%20Policy%20Gradients.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"images/epolicy.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Value based: Here we learn a value function that maps a state to a action. It is useful when we have finite action space. \n",
    "- Policy Based: Here we directly try to learn the optimal policy using the value function. It is useful when we have a coninuous or stochastic actions. \n",
    "# Actor Critic Method\n",
    "A hybrid between value-based algorithms and policy based algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what is Actor and Critic \n",
    "1. The **Critic** estimates the value function. Which could be either an action-value (Q-value) or a State-value(Value)\n",
    "$$ q\\hat (s,a,w) $$\n",
    "1. The **Actor** updates the policy distribution i the direction suggested by the critic, which is the policy gradients\n",
    "$$\\pi(s,a,\\theta)$$\n",
    "As we can see here we have 2 neural networks here!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Abhishek\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Abhishek\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Abhishek\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Abhishek\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Abhishek\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Abhishek\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.layers.merge import Add, Multiply\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the ActorCritic Class\n",
    "- Chain rule: find the gradient of chaging the actor network params in  #\n",
    "- getting closest to the final value network predictions, i.e. de/dA    #\n",
    "- Calculate de/dA as = de/dC * dC/dA, where e is error, C critic, A act "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic:\n",
    "    def __init__(self, env, sess):\n",
    "        self.env  = env\n",
    "        self.sess = sess\n",
    "\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = .995\n",
    "        self.gamma = .95\n",
    "        self.tau   = .125\n",
    "\n",
    "\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.actor_state_input, self.actor_model = self.create_actor_model()\n",
    "        _, self.target_actor_model = self.create_actor_model()\n",
    "        \n",
    " # this is where we will feed from critic\n",
    "\n",
    "        self.actor_critic_grad = tf.placeholder(tf.float32, \n",
    "            [None, self.env.action_space.shape[0]])\n",
    "\n",
    "        actor_model_weights = self.actor_model.trainable_weights\n",
    "        self.actor_grads = tf.gradients(self.actor_model.output, \n",
    "            actor_model_weights, -self.actor_critic_grad) # dC/dA (from actor)\n",
    "        grads = zip(self.actor_grads, actor_model_weights)\n",
    "        self.optimize = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(grads)\n",
    "\n",
    "##the critic model will help us check the perfoamnce by actor\n",
    "\n",
    "        self.critic_state_input, self.critic_action_input, \\\n",
    "            self.critic_model = self.create_critic_model()\n",
    "        _, _, self.target_critic_model = self.create_critic_model()\n",
    "\n",
    "        self.critic_grads = tf.gradients(self.critic_model.output, \n",
    "            self.critic_action_input) # where we calcaulte de/dC for feeding above\n",
    "\n",
    "        # Initialize for later gradient calculations\n",
    "        self.sess.run(tf.initialize_all_variables())\n",
    "\n",
    "## Model definations\n",
    "\n",
    "\n",
    "    # actor model\n",
    "    ## In a current state what is the best action\n",
    "    def create_actor_model(self):\n",
    "        state_input = Input(shape=self.env.observation_space.shape)\n",
    "        h1 = Dense(24, activation='relu')(state_input)\n",
    "        h2 = Dense(48, activation='relu')(h1)\n",
    "        h3 = Dense(24, activation='relu')(h2)\n",
    "        output = Dense(self.env.action_space.shape[0], activation='relu')(h3)\n",
    "\n",
    "        model = Model(input=state_input, output=output)\n",
    "        adam  = Adam(lr=0.001)\n",
    "        model.compile(loss=\"mse\", optimizer=adam)\n",
    "        return state_input, model\n",
    "\n",
    "    # critic model\n",
    "    # the q scores are calculated seperately in the critic model \n",
    "    # It input the action space and state space, and outputs the value\n",
    "    def create_critic_model(self):\n",
    "        state_input = Input(shape=self.env.observation_space.shape)\n",
    "        state_h1 = Dense(24, activation='relu')(state_input)\n",
    "        state_h2 = Dense(48)(state_h1)\n",
    "\n",
    "        action_input = Input(shape=self.env.action_space.shape)\n",
    "        action_h1    = Dense(48)(action_input)\n",
    "        \n",
    "        # a layer in the middle to merge the two\n",
    "        merged    = Add()([state_h2, action_h1])\n",
    "        merged_h1 = Dense(24, activation='relu')(merged)\n",
    "        output = Dense(1, activation='relu')(merged_h1)\n",
    "        model  = Model(input=[state_input,action_input], output=output)\n",
    "\n",
    "        adam  = Adam(lr=0.001)\n",
    "        model.compile(loss=\"mse\", optimizer=adam)\n",
    "        return state_input, action_input, model\n",
    "\n",
    "# Model Training\n",
    "# the updates are happenening at every time step\n",
    "\n",
    "    # this is our memory\n",
    "    def remember(self, cur_state, action, reward, new_state, done):\n",
    "        self.memory.append([cur_state, action, reward, new_state, done])\n",
    "\n",
    "        ## lets trainig the actor\n",
    "    def _train_actor(self, samples):\n",
    "        for sample in samples:\n",
    "            cur_state, action, reward, new_state, _ = sample\n",
    "            predicted_action = self.actor_model.predict(cur_state)\n",
    "            grads = self.sess.run(self.critic_grads, feed_dict={\n",
    "                self.critic_state_input:  cur_state,\n",
    "                self.critic_action_input: predicted_action\n",
    "            })[0]\n",
    "\n",
    "            self.sess.run(self.optimize, feed_dict={\n",
    "                self.actor_state_input: cur_state,\n",
    "                self.actor_critic_grad: grads\n",
    "            })\n",
    "\n",
    "    ## lets trainig the critic \n",
    "    def _train_critic(self, samples):\n",
    "        for sample in samples:\n",
    "            cur_state, action, reward, new_state, done = sample\n",
    "            if not done:\n",
    "                target_action = self.target_actor_model.predict(new_state)\n",
    "                future_reward = self.target_critic_model.predict(\n",
    "                    [new_state, target_action])[0][0]\n",
    "                reward += self.gamma * future_reward\n",
    "            self.critic_model.fit([cur_state, action], reward, verbose=0)\n",
    "\n",
    "    def train(self):\n",
    "        batch_size = 32\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        rewards = []\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        self._train_critic(samples)\n",
    "        self._train_actor(samples)\n",
    "\n",
    "## Target Model Updating \n",
    "##  we want to determine what change in parameters (in the actor model) \n",
    "##  would result in the largest increase in the Q value (predicted by the critic model)\n",
    "\n",
    "    def _update_actor_target(self):\n",
    "        actor_model_weights  = self.actor_model.get_weights()\n",
    "        actor_target_weights = self.target_critic_model.get_weights()\n",
    "\n",
    "        for i in range(len(actor_target_weights)):\n",
    "            actor_target_weights[i] = actor_model_weights[i]\n",
    "        self.target_critic_model.set_weights(actor_target_weights)\n",
    "\n",
    "    def _update_critic_target(self):\n",
    "        critic_model_weights  = self.critic_model.get_weights()\n",
    "        critic_target_weights = self.critic_target_model.get_weights()\n",
    "\n",
    "        for i in range(len(critic_target_weights)):\n",
    "            critic_target_weights[i] = critic_model_weights[i]\n",
    "        self.critic_target_model.set_weights(critic_target_weights)\n",
    "\n",
    "    def update_target(self):\n",
    "        self._update_actor_target()\n",
    "        self._update_critic_target()\n",
    "\n",
    "## Model predictions\n",
    "    # similar to DQN \n",
    "    def act(self, cur_state):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        return self.actor_model.predict(cur_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "We will be using i\n",
    "The inverted pendulum swingup problem is a classic problem in the control literature. In this version of the problem, the pendulum starts in a random position, and the goal is to swing it up so it stays upright.\n",
    "The pendulum Env has a infinite input space!! the number of action space in inf!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:52: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "C:\\Users\\Abhishek\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:71: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  [[1.4859121]] Reward:  [-3.8274097]\n",
      "Action:  [[-0.53925604]] Reward:  [-3.874733]\n",
      "Action:  [[1.7384005]] Reward:  [-4.1783094]\n",
      "Action:  [[0.79900706]] Reward:  [-4.572535]\n",
      "Action:  [[-1.101666]] Reward:  [-5.1743703]\n",
      "Action:  [[-1.339058]] Reward:  [-6.159809]\n",
      "Action:  [[-0.6874918]] Reward:  [-7.46648]\n",
      "Action:  [[1.7752327]] Reward:  [-8.936607]\n",
      "Action:  [[-1.9174204]] Reward:  [-10.128922]\n",
      "Action:  [[-1.8442848]] Reward:  [-11.905601]\n",
      "Action:  [[-0.3645895]] Reward:  [-10.792521]\n",
      "Action:  [[-0.7449808]] Reward:  [-9.361925]\n",
      "Action:  [[-1.5841984]] Reward:  [-8.004094]\n",
      "Action:  [[-0.68719226]] Reward:  [-6.795425]\n",
      "Action:  [[0.]] Reward:  [-5.599224]\n",
      "Action:  [[-1.9638232]] Reward:  [-4.5368032]\n",
      "Action:  [[-1.933766]] Reward:  [-3.8076043]\n",
      "Action:  [[-1.4386117]] Reward:  [-3.207256]\n",
      "Action:  [[-0.30811134]] Reward:  [-2.7241197]\n",
      "Action:  [[-1.2116411]] Reward:  [-2.3834014]\n",
      "Action:  [[0.8065858]] Reward:  [-2.2379467]\n",
      "Action:  [[-0.10481944]] Reward:  [-2.3048043]\n",
      "Action:  [[0.12364951]] Reward:  [-2.604233]\n",
      "Action:  [[0.29613847]] Reward:  [-3.1584594]\n",
      "Action:  [[1.4944514]] Reward:  [-4.0050855]\n",
      "Action:  [[-1.2534667]] Reward:  [-5.316284]\n",
      "Action:  [[1.9982374]] Reward:  [-6.5864077]\n",
      "Action:  [[0.21232945]] Reward:  [-8.607975]\n",
      "Action:  [[1.6974192]] Reward:  [-10.606831]\n",
      "Action:  [[0.33244163]] Reward:  [-13.030199]\n",
      "Action:  [[-0.50402987]] Reward:  [-12.522176]\n",
      "Action:  [[1.1962298]] Reward:  [-10.555731]\n",
      "Action:  [[-0.920824]] Reward:  [-8.876751]\n",
      "Action:  [[-1.9512905]] Reward:  [-6.988477]\n",
      "Action:  [[1.5974296]] Reward:  [-5.262362]\n",
      "Action:  [[0.48072]] Reward:  [-4.2151537]\n",
      "Action:  [[0.2742664]] Reward:  [-3.2636874]\n",
      "Action:  [[-0.64622253]] Reward:  [-2.5325847]\n",
      "Action:  [[-0.03720457]] Reward:  [-1.9973204]\n",
      "Action:  [[0.]] Reward:  [-1.722623]\n",
      "Action:  [[-1.6865299]] Reward:  [-1.6529536]\n",
      "Action:  [[0.02617463]] Reward:  [-1.8471287]\n",
      "Action:  [[-0.84442186]] Reward:  [-2.2522285]\n",
      "Action:  [[-0.7029202]] Reward:  [-2.9746146]\n",
      "Action:  [[1.7662975]] Reward:  [-4.016723]\n",
      "Action:  [[-1.0007974]] Reward:  [-5.044389]\n",
      "Action:  [[-1.3256553]] Reward:  [-6.719167]\n",
      "Action:  [[0.4319413]] Reward:  [-8.820319]\n",
      "Action:  [[0.]] Reward:  [-10.846089]\n",
      "Action:  [[-0.40485984]] Reward:  [-13.032245]\n",
      "Action:  [[1.44252]] Reward:  [-12.824776]\n",
      "Action:  [[1.109903]] Reward:  [-10.674314]\n",
      "Action:  [[0.43039927]] Reward:  [-8.67372]\n",
      "Action:  [[1.064524]] Reward:  [-6.9436116]\n",
      "Action:  [[-1.6857111]] Reward:  [-5.3786287]\n",
      "Action:  [[0.09451315]] Reward:  [-4.3456483]\n",
      "Action:  [[-1.9566863]] Reward:  [-3.3604765]\n",
      "Action:  [[1.1268257]] Reward:  [-2.7322278]\n",
      "Action:  [[-0.9756676]] Reward:  [-2.1371877]\n",
      "Action:  [[0.39429462]] Reward:  [-1.851904]\n",
      "Action:  [[-1.1287732]] Reward:  [-1.7328069]\n",
      "Action:  [[0.]] Reward:  [-1.7869122]\n",
      "Action:  [[1.2058603]] Reward:  [-2.040328]\n",
      "Action:  [[1.0187777]] Reward:  [-2.6105237]\n",
      "Action:  [[0.7101213]] Reward:  [-3.5035627]\n",
      "Action:  [[1.037607]] Reward:  [-4.721576]\n",
      "Action:  [[-0.47724035]] Reward:  [-6.359498]\n",
      "Action:  [[-1.0740899]] Reward:  [-8.088108]\n",
      "Action:  [[0.8564427]] Reward:  [-9.892898]\n",
      "Action:  [[0.9201844]] Reward:  [-12.191651]\n",
      "Action:  [[-0.1166716]] Reward:  [-13.530919]\n",
      "Action:  [[0.92039573]] Reward:  [-11.579771]\n",
      "Action:  [[0.72777647]] Reward:  [-9.775011]\n",
      "Action:  [[-1.5003748]] Reward:  [-7.9824452]\n",
      "Action:  [[0.5877861]] Reward:  [-6.047954]\n",
      "Action:  [[1.7701429]] Reward:  [-4.691361]\n",
      "Action:  [[0.3953839]] Reward:  [-3.6819882]\n",
      "Action:  [[-0.68264455]] Reward:  [-2.750581]\n",
      "Action:  [[-0.2272785]] Reward:  [-1.9927279]\n",
      "Action:  [[-0.12864058]] Reward:  [-1.511069]\n",
      "Action:  [[0.]] Reward:  [-1.2301277]\n",
      "Action:  [[0.]] Reward:  [-1.1197597]\n",
      "Action:  [[-0.59108585]] Reward:  [-1.1650945]\n",
      "Action:  [[1.5815858]] Reward:  [-1.4013824]\n",
      "Action:  [[0.89965326]] Reward:  [-1.6994835]\n",
      "Action:  [[-1.4294434]] Reward:  [-2.1765826]\n",
      "Action:  [[1.016279]] Reward:  [-3.088572]\n",
      "Action:  [[0.]] Reward:  [-4.055851]\n",
      "Action:  [[0.]] Reward:  [-5.4333334]\n",
      "Action:  [[0.]] Reward:  [-7.139019]\n",
      "Action:  [[-0.8465493]] Reward:  [-9.140365]\n",
      "Action:  [[0.78561443]] Reward:  [-11.543651]\n",
      "Action:  [[1.3671589]] Reward:  [-13.68063]\n",
      "Action:  [[0.]] Reward:  [-12.542092]\n",
      "Action:  [[-0.5464142]] Reward:  [-10.602501]\n",
      "Action:  [[-0.55192983]] Reward:  [-8.775317]\n",
      "Action:  [[1.1154648]] Reward:  [-7.0541515]\n",
      "Action:  [[0.]] Reward:  [-5.3270473]\n",
      "Action:  [[-0.23646314]] Reward:  [-4.0496736]\n",
      "Action:  [[-0.92749745]] Reward:  [-3.0680332]\n",
      "Action:  [[1.1306874]] Reward:  [-2.3713777]\n",
      "Action:  [[-0.9978581]] Reward:  [-1.7825568]\n",
      "Action:  [[1.5722271]] Reward:  [-1.4970694]\n",
      "Action:  [[-0.1243876]] Reward:  [-1.354149]\n",
      "Action:  [[0.]] Reward:  [-1.4321996]\n",
      "Action:  [[-1.8832748]] Reward:  [-1.6961466]\n",
      "Action:  [[-1.5031809]] Reward:  [-2.0220592]\n",
      "Action:  [[1.2250984]] Reward:  [-2.5002444]\n",
      "Action:  [[0.49358547]] Reward:  [-3.435507]\n",
      "Action:  [[-0.19914559]] Reward:  [-4.6541786]\n",
      "Action:  [[-0.90999705]] Reward:  [-6.1134057]\n",
      "Action:  [[0.78921956]] Reward:  [-7.730605]\n",
      "Action:  [[1.1006634]] Reward:  [-9.89012]\n",
      "Action:  [[0.9623362]] Reward:  [-12.333245]\n",
      "Action:  [[0.99996376]] Reward:  [-13.990976]\n",
      "Action:  [[0.9938987]] Reward:  [-12.141573]\n",
      "Action:  [[-0.30449334]] Reward:  [-10.223664]\n",
      "Action:  [[0.8155808]] Reward:  [-8.138154]\n",
      "Action:  [[0.69263065]] Reward:  [-6.424056]\n",
      "Action:  [[0.58105206]] Reward:  [-4.9225917]\n",
      "Action:  [[-0.01780394]] Reward:  [-3.6778574]\n",
      "Action:  [[0.39906877]] Reward:  [-2.6475747]\n",
      "Action:  [[0.3297293]] Reward:  [-1.9126132]\n",
      "Action:  [[1.6371143]] Reward:  [-1.3745071]\n",
      "Action:  [[0.17192344]] Reward:  [-1.0495096]\n",
      "Action:  [[0.08758134]] Reward:  [-0.75340843]\n",
      "Action:  [[0.02939514]] Reward:  [-0.55898446]\n",
      "Action:  [[-1.3084692]] Reward:  [-0.45009276]\n",
      "Action:  [[-0.25470415]] Reward:  [-0.42319828]\n",
      "Action:  [[0.]] Reward:  [-0.49419063]\n",
      "Action:  [[0.]] Reward:  [-0.64474106]\n",
      "Action:  [[0.36463255]] Reward:  [-0.89434654]\n",
      "Action:  [[0.]] Reward:  [-1.2443874]\n",
      "Action:  [[-1.3879572]] Reward:  [-1.779604]\n",
      "Action:  [[0.9770516]] Reward:  [-2.6956487]\n",
      "Action:  [[1.1096183]] Reward:  [-3.6477308]\n",
      "Action:  [[0.]] Reward:  [-4.8526583]\n",
      "Action:  [[0.8627377]] Reward:  [-6.5566845]\n",
      "Action:  [[0.]] Reward:  [-8.434632]\n",
      "Action:  [[-0.37765232]] Reward:  [-10.739038]\n",
      "Action:  [[0.]] Reward:  [-13.297678]\n",
      "Action:  [[0.]] Reward:  [-14.373518]\n",
      "Action:  [[0.]] Reward:  [-12.2226925]\n",
      "Action:  [[-0.5855211]] Reward:  [-10.053927]\n",
      "Action:  [[0.]] Reward:  [-8.080725]\n",
      "Action:  [[0.]] Reward:  [-6.209141]\n",
      "Action:  [[0.]] Reward:  [-4.634701]\n",
      "Action:  [[0.77363926]] Reward:  [-3.3790636]\n",
      "Action:  [[0.]] Reward:  [-2.3531823]\n",
      "Action:  [[-1.0787821]] Reward:  [-1.6722205]\n",
      "Action:  [[1.8687842]] Reward:  [-1.246656]\n",
      "Action:  [[0.04462195]] Reward:  [-0.82820994]\n",
      "Action:  [[1.142318]] Reward:  [-0.6501653]\n",
      "Action:  [[0.]] Reward:  [-0.57887125]\n",
      "Action:  [[0.]] Reward:  [-0.6266712]\n",
      "Action:  [[0.00892692]] Reward:  [-0.7738135]\n",
      "Action:  [[0.10859469]] Reward:  [-1.0379789]\n",
      "Action:  [[0.2597662]] Reward:  [-1.4559801]\n",
      "Action:  [[0.43437546]] Reward:  [-2.0826135]\n",
      "Action:  [[0.6057837]] Reward:  [-2.9884918]\n",
      "Action:  [[0.73489755]] Reward:  [-4.252257]\n",
      "Action:  [[-1.5892788]] Reward:  [-5.942124]\n",
      "Action:  [[0.8965757]] Reward:  [-7.581181]\n",
      "Action:  [[-0.7254315]] Reward:  [-9.994835]\n",
      "Action:  [[1.050243]] Reward:  [-12.293596]\n",
      "Action:  [[1.0989304]] Reward:  [-15.068577]\n",
      "Action:  [[0.21525133]] Reward:  [-13.414188]\n",
      "Action:  [[1.0434494]] Reward:  [-11.184677]\n",
      "Action:  [[0.95829487]] Reward:  [-9.144782]\n",
      "Action:  [[-1.8603439]] Reward:  [-7.2302914]\n",
      "Action:  [[0.52919596]] Reward:  [-5.145485]\n",
      "Action:  [[0.5787561]] Reward:  [-3.8170445]\n",
      "Action:  [[0.508606]] Reward:  [-2.7897727]\n",
      "Action:  [[-0.11363018]] Reward:  [-2.011326]\n",
      "Action:  [[0.97577274]] Reward:  [-1.3941258]\n",
      "Action:  [[0.30617294]] Reward:  [-1.0285213]\n",
      "Action:  [[0.24145433]] Reward:  [-0.7291231]\n",
      "Action:  [[0.18486178]] Reward:  [-0.5175838]\n",
      "Action:  [[0.13688913]] Reward:  [-0.36991858]\n",
      "Action:  [[0.33197835]] Reward:  [-0.26856467]\n",
      "Action:  [[0.09266248]] Reward:  [-0.20952678]\n",
      "Action:  [[1.9610392]] Reward:  [-0.17089535]\n",
      "Action:  [[0.08726846]] Reward:  [-0.22040755]\n",
      "Action:  [[0.07145779]] Reward:  [-0.22288825]\n",
      "Action:  [[0.4668043]] Reward:  [-0.25330904]\n",
      "Action:  [[0.09791446]] Reward:  [-0.33761105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  [[-1.9295152]] Reward:  [-0.45431828]\n",
      "Action:  [[0.13504395]] Reward:  [-0.49356747]\n",
      "Action:  [[0.21508798]] Reward:  [-0.71515334]\n",
      "Action:  [[0.41877177]] Reward:  [-1.0460356]\n",
      "Action:  [[0.4753767]] Reward:  [-1.5458674]\n",
      "Action:  [[0.61697686]] Reward:  [-2.2694473]\n",
      "Action:  [[0.7761104]] Reward:  [-3.308586]\n",
      "Action:  [[0.89899606]] Reward:  [-4.762735]\n",
      "Action:  [[1.007802]] Reward:  [-6.714568]\n",
      "Action:  [[-0.90380746]] Reward:  [-9.20199]\n",
      "Action:  [[1.1698376]] Reward:  [-11.640985]\n",
      "Action:  [[-1.4752415]] Reward:  [-14.653433]\n",
      "Action:  [[-1.2874469]] Reward:  [-15.437494]\n",
      "Action:  [[1.1239733]] Reward:  [-12.814799]\n"
     ]
    }
   ],
   "source": [
    "# determines how to assign values to each state, i.e. takes the state\n",
    "# and action (two-input model) and determines the corresponding value\n",
    "\n",
    "def main():\n",
    "    sess = tf.Session()\n",
    "    K.set_session(sess)\n",
    "    env = gym.make(\"Pendulum-v0\")\n",
    "    actor_critic = ActorCritic(env, sess)\n",
    "    \n",
    "    # Hyper parameters\n",
    "    num_trials = 1000\n",
    "    trial_len  = 500\n",
    "\n",
    "    cur_state = env.reset()\n",
    "    \n",
    "    # sample random actions\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "#         env.render()\n",
    "        \n",
    "        # current state \n",
    "        cur_state = cur_state.reshape((1, env.observation_space.shape[0]))\n",
    "        \n",
    "        # the acord learn the steps\n",
    "        action = actor_critic.act(cur_state)\n",
    "        action = action.reshape((1, env.action_space.shape[0]))\n",
    "        \n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        new_state = new_state.reshape((1, env.observation_space.shape[0]))\n",
    "        print(\"Action: \", action, \"Reward: \", reward)\n",
    "        \n",
    "        actor_critic.remember(cur_state, action, reward, new_state, done)\n",
    "        actor_critic.train()\n",
    "\n",
    "        cur_state = new_state\n",
    "        \n",
    "        if done ==True:\n",
    "            break\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- A critic measures how good the action taken is, \"value-based\", where as an \"Actor\" controls how the agent behaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources:\n",
    "1. https://www.freecodecamp.org/news/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f/\n",
    "1. https://towardsdatascience.com/policy-gradients-in-a-nutshell-8b72f9743c5d\n",
    "1. https://towardsdatascience.com/reinforcement-learning-w-keras-openai-actor-critic-models-f084612cfd69"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
