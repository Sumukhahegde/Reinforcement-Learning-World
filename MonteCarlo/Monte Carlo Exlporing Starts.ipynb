{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Exploring starts\n",
    "\n",
    "### Abstract:\n",
    "\n",
    "This notebook contains the implementation of Monte Carlo exploring starts for Grid world game.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: https://github.com/lazyprogrammer/machine_learning_examples/tree/master/rl\n",
    "# the Monte Carlo Exploring-Starts method to find the optimal policy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gridWorldGame import standard_grid, negative_grid,print_values, print_policy\n",
    "\n",
    "SMALL_ENOUGH = 1e-3\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(grid, policy):\n",
    "  # returns a list of states and corresponding returns\n",
    "  # we have a deterministic policy\n",
    "  # we would never end up at certain states, but we still want to measure their value\n",
    "  # this is called the \"exploring starts\" method\n",
    "  start_states = list(grid.actions.keys())\n",
    "  start_idx = np.random.choice(len(start_states))\n",
    "  grid.set_state(start_states[start_idx])\n",
    "\n",
    "  s = grid.current_state()\n",
    "  a = np.random.choice(ALL_POSSIBLE_ACTIONS) # first action is uniformly random\n",
    "\n",
    "  # each triple s(t), a(t), r(t)\n",
    "  # but r(t) results from taking action a(t-1) from s(t-1) and landing in s(t)\n",
    "  states_actions_rewards = [(s, a, 0)]\n",
    "  seen_states = set()\n",
    "  seen_states.add(grid.current_state())\n",
    "  num_steps = 0\n",
    "  while True:\n",
    "    r = grid.move(a)\n",
    "    num_steps += 1\n",
    "    s = grid.current_state()\n",
    "\n",
    "    if s in seen_states:\n",
    "      # we don't end up in an infinitely long episode\n",
    "      # bumping into the wall repeatedly\n",
    "      reward = -10. / num_steps\n",
    "      states_actions_rewards.append((s, None, reward))\n",
    "      break\n",
    "    elif grid.game_over():\n",
    "      states_actions_rewards.append((s, None, r))\n",
    "      break\n",
    "    else:\n",
    "      a = policy[s]\n",
    "      states_actions_rewards.append((s, a, r))\n",
    "    seen_states.add(s)\n",
    "\n",
    "  # calculate the returns by working backwards from the terminal state\n",
    "  G = 0\n",
    "  states_actions_returns = []\n",
    "  first = True\n",
    "  for s, a, r in reversed(states_actions_rewards):\n",
    "    # the value of the terminal state is 0 by definition\n",
    "    # we should ignore the first state we encounter\n",
    "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "    if first:\n",
    "      first = False\n",
    "    else:\n",
    "      states_actions_returns.append((s, a, G))\n",
    "    G = r + GAMMA*G\n",
    "  states_actions_returns.reverse() # we want it to be in order of state visited\n",
    "  return states_actions_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_dict(d):\n",
    "  # returns the argmax (key) and max (value) from a dictionary\n",
    "  max_key = None\n",
    "  max_val = float('-inf')\n",
    "  for k, v in d.items():\n",
    "    if v > max_val:\n",
    "      max_val = v\n",
    "      max_key = k\n",
    "  return max_key, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      "-0.90|-0.90|-0.90| 1.00|\n",
      "---------------------------\n",
      "-0.90| 0.00|-0.90|-1.00|\n",
      "---------------------------\n",
      "-0.90|-0.90|-0.90|-0.90|\n"
     ]
    }
   ],
   "source": [
    "grid = negative_grid(step_cost=-0.9)\n",
    "# print rewards\n",
    "print(\"rewards:\")\n",
    "print_values(grid.rewards, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial policy:\n",
      "---------------------------\n",
      "  R  |  L  |  D  |     |\n",
      "---------------------------\n",
      "  R  |     |  R  |     |\n",
      "---------------------------\n",
      "  R  |  L  |  D  |  U  |\n"
     ]
    }
   ],
   "source": [
    "# state -> action\n",
    "# initialize a random policy\n",
    "policy = {}\n",
    "for s in grid.actions.keys():\n",
    "  policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "# initial policy\n",
    "print(\"initial policy:\")\n",
    "print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): {'U': 0, 'D': 0, 'L': 0, 'R': 0}, (2, 1): {'U': 0, 'D': 0, 'L': 0, 'R': 0}, (2, 3): {'U': 0, 'D': 0, 'L': 0, 'R': 0}, (1, 0): {'U': 0, 'D': 0, 'L': 0, 'R': 0}, (0, 1): {'U': 0, 'D': 0, 'L': 0, 'R': 0}, (1, 2): {'U': 0, 'D': 0, 'L': 0, 'R': 0}, (2, 0): {'U': 0, 'D': 0, 'L': 0, 'R': 0}, (2, 2): {'U': 0, 'D': 0, 'L': 0, 'R': 0}, (0, 2): {'U': 0, 'D': 0, 'L': 0, 'R': 0}}\n"
     ]
    }
   ],
   "source": [
    "# initialize Q(s,a) and returns\n",
    "Q = {}\n",
    "returns = {} # dictionary of state -> list of returns we've received\n",
    "states = grid.all_states()\n",
    "for s in states:\n",
    "  if s in grid.actions: # not a terminal state\n",
    "    Q[s] = {}\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "      Q[s][a] = 0 # needs to be initialized to something so we can argmax it\n",
    "      returns[(s,a)] = []\n",
    "  else:\n",
    "    # terminal state or state we can't otherwise get to\n",
    "    pass\n",
    "\n",
    "# initial Q values for all states in grid\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat until convergence\n",
    "deltas = []\n",
    "for t in range(2000):\n",
    "  # generate an episode using pi\n",
    "  biggest_change = 0\n",
    "  states_actions_returns = play_game(grid, policy)\n",
    "  seen_state_action_pairs = set()\n",
    "  for s, a, G in states_actions_returns:\n",
    "    # check if we have already seen s\n",
    "    # called \"first-visit\" MC policy evaluation\n",
    "    sa = (s, a)\n",
    "    if sa not in seen_state_action_pairs:\n",
    "      old_q = Q[s][a]\n",
    "      returns[sa].append(G)\n",
    "      Q[s][a] = np.mean(returns[sa])\n",
    "      biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
    "      seen_state_action_pairs.add(sa)\n",
    "  deltas.append(biggest_change)\n",
    "\n",
    "  # update policy\n",
    "  for s in policy.keys():\n",
    "    policy[s] = max_dict(Q[s])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  U  |\n"
     ]
    }
   ],
   "source": [
    "plt.plot(deltas)\n",
    "plt.show()\n",
    "\n",
    "print(\"final policy:\")\n",
    "print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final values:\n",
      "---------------------------\n",
      "-1.74|-0.79| 1.00| 0.00|\n",
      "---------------------------\n",
      "-2.50| 0.00|-0.73| 0.00|\n",
      "---------------------------\n",
      "-3.24|-2.96|-1.89|-1.00|\n"
     ]
    }
   ],
   "source": [
    "# find V\n",
    "V = {}\n",
    "for s, Qs in Q.items():\n",
    "  V[s] = max_dict(Q[s])[1]\n",
    "\n",
    "print(\"final values:\")\n",
    "print_values(V, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "The above cell shows the final values for Monte Carlo exploring starts for Gird world game\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
