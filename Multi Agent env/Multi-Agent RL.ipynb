{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where do we see it?\n",
    "\n",
    "Multi agent reinforcement learning is a type of Reinforcement Learning which involves more than one agent interacting with an environment.\n",
    "There are many examples of Multi Agent systems around us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be it early morning with all the cars going to work\n",
    "<img src=\"image/cars_on_the_street.png\" alt=\"Cars on the Street\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or your soccer players on the field\n",
    "<img src=\"image/soccer.png\" alt=\"Soccer Players\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or Bees working inside a honeycomb\n",
    "<img src=\"image/bees.png\" alt=\"Honeybees in a honeycomb\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a case where an autonomous car is driving you to office. And its goal is to safely take you to the office in time\n",
    "<img src=\"image/autonomous_car.png\" alt=\"Autonomous Cars\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anytime it has to accelerate, break or change lanes....\n",
    "<img src=\"image/car_learning.png\" alt=\"car_learning\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does so by considering the other cars in it vicinity\n",
    "<img src=\"image/car_overtaking.png\"  alt=\"Car Overtaking\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other cars are trying to do the same as they get more and more driving experience\n",
    "If you contrast this to a scenario where the car is alone on the street and driving around, it can go as fast as possible so the task of driving becomes relatively simpler and so the agent does not ever learn the complications that come with driving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nothing but a Multi Agent system where multiple agents interact with one another. In a multi-agent system each agent may or may not know about the other agents that are present in the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Game Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a single RL Drone Agent and its task is to pickup a package\n",
    "<img src=\"image/drone1.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has a set of possible actions that it can take.\n",
    "\n",
    "It can go Right\n",
    "<img src=\"image/drone2.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left\n",
    "<img src=\"image/drone3.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up\n",
    "<img src=\"image/drone4.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Down\n",
    "<img src=\"image/drone5.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And Grasping\n",
    "<img src=\"image/drone6.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we decide that we give it a reward of +50 if it picks up the package\n",
    "<img src=\"image/drone7.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And -1 if it drops the package\n",
    "<img src=\"image/drone8.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the difference in Multi Agent RL is that we have another agent, in this case another drone in our environment.\n",
    "And now both the drones will collectively try to grasp the package.\n",
    "They're both trying to observe the package from their own perspective.\n",
    "<img src=\"image/drone9.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They both have their own policies that returned an action for their observations\n",
    "<img src=\"image/drone10.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both also have their own set of actions\n",
    "<img src=\"image/drone12.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main thing about Multi-Agent RL is that they have a joint set of actions that they can take to interact in their environment. Both the left drone and the right drone must begin the action.\n",
    "<img src=\"image/drone13.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the pair DL is when the left drone moves down and the right drone moves to the left\n",
    "<img src=\"image/drone15.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example illustrates the Markov Game Framework\n",
    "\n",
    "A Markov Game is a tuple written as this\n",
    "<img src=\"image/drone16.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n: Number of Agents\n",
    "\n",
    "S: Set of states in the environment\n",
    "\n",
    "Ai: A is the set of actions in the environment by agent i\n",
    "\n",
    "Oi: O is the set of observations in the environment by agent i\n",
    "\n",
    "Ri: R is the set of rewards for the actions taken in the environment by agent i\n",
    "\n",
    "$\\pi$i: $\\pi$ is the set of policy of agent i\n",
    "\n",
    "T: T is Transition Function, given the current state and joint action it gives a probability distribution over the set of possible states\n",
    "\n",
    "<img src=\"image/markov_eqn.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even here, the State transitions are Markovian, that is the MDP only depends upon the current state and the action take in this state.\n",
    "However, when it comes to the Transition function, it depends upon the Joint action taken in the current state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Agent Environment Types:\n",
    "\n",
    "* **Cooperative Environments**: \n",
    "\n",
    "An environment where multiple agents have to accomplish a goal by working together is called a cooperative environment. In this kind of environment, when both agents are able to successfully complete a task together, they are rewarded and so both of them learn to cooperate in the environment. A good example for this kind of environment is the example we covered above where two drones have to work together to pick a package and deliver it.\n",
    "\n",
    "* **Competitive Environments**: \n",
    "\n",
    "An environment where multiple agents have to compete with each other to reach their goal is called a Competitive Environment. In this kind of environment, each agent is tasked with a similar goal which can only be achieved by one of them and the agent that is able to achieve the goal is rewarded. This reinforces the idea of competing with the other agent in the environment. A good example for this kind of environment is one you will see an implementation of very soon, which is the Tennis Environment in which we work with two agents present on opposite sides of the net and both are tasked with not letting the ball drop on their side. And if the ball drops on the opponent's side then they are rewarded.\n",
    "\n",
    "* **Mixed Environments**: \n",
    "\n",
    "An environment where multiple agents have to both cooperate and compete  with each other by interacting with their environment is called a Mixed Environment. In this kind of environment, each agent is tasked with achieving a goal for which they not only have to cooperate with other agents in the environment but also compete with them. Here depending upon which has more preference, you will assign higher rewards for the kind of action you prefer your agent to fulfill. Say, giving higher positive reward to cooperation and lower positive reward to competition. In this kind of setting the agents would cooperate more and compete less, but they will do both the actions. A good example for Mixed Environment is a Traffic Control setting, where each agent is tasked with reaching their goal as fast as possible. For a Traffic Control environment, each agent has to adhere to the traffic rules and make sure that it does not crash into other agents while driving, but at the same time they have to overtake the other agents in order to reach their goal faster while driving within the speed limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2020 Sonali Vinodkumar Singh\n",
    "\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}